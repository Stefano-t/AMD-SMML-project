{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-dataframe_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stefano-t/AMD-SMML-project/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPqEOENsXxDN"
      },
      "source": [
        "# Install Kaggle and download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6ls_Hc-R8P6"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "!echo '{\"username\":\"stefanotaverni\",\"key\":\"c22f4c36169267251f92367e36c7d6fc\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d census/2013-american-community-survey\n",
        "!mkdir /content/data\n",
        "print(\"unzip folder...\")\n",
        "!unzip /content/2013-american-community-survey.zip -d /content/data > /dev/null\n",
        "print(\"download completed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq3vtPmJX-kj"
      },
      "source": [
        "# Download Spark and setup the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w80wHaf0ShdY"
      },
      "source": [
        "!rm spark-3.0.1-bin-hadoop2.7.tgz*\n",
        "print(\"update system and install jdk 8\")\n",
        "!apt-get update > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "print(\"download and extract Spark...\")\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.1-bin-hadoop2.7.tgz > /dev/null\n",
        "!pip install -q findspark\n",
        "\n",
        "print(\"setup Spark...\")\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init(\"spark-3.0.1-bin-hadoop2.7\")# SPARK_HOME\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark_driver_memory = \"8g\"\n",
        "spark_executor_memory = \"1g\"\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "                    .config(\"spark.driver.memory\", spark_driver_memory) \\\n",
        "                    .config(\"spark.execuror.memory\", spark_executor_memory) \\\n",
        "                    .master(\"local[*]\") \\\n",
        "                    .getOrCreate()\n",
        "print(\"Spark session created\")\n",
        "import pyspark\n",
        "sc = spark.sparkContext\n",
        "print(\"Spark context created\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcHF5H32XT4i"
      },
      "source": [
        "# VARIABLES\n",
        "Definition of the main variables to work with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbyFmDhWXT4k"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWNWM6cUXT4l"
      },
      "source": [
        "dataset_to_load = \"ss13pusa.csv\"\n",
        "dataset_sample_fraction = 0.1\n",
        "dataset_sample_seed = 1965\n",
        "\"\"\"Determine how to divide the loaded dataset.\n",
        "\n",
        "The fraction must be a value > 0 and <= 1, and the seed determines the random\n",
        "seed used to split the starting dataset.\n",
        "\"\"\"\n",
        "\n",
        "target_label = \"PINCP\"\n",
        "forbidden_columns = [\"PERNP\", \"WIGP\"]\n",
        "\"\"\"Label to predict and columns to remove.\"\"\"\n",
        "\n",
        "\n",
        "split_weights = [0.9, 0.1]\n",
        "split_seed = 12345\n",
        "\"\"\"Determine how to split the pre-processed Dataframe.\n",
        "\n",
        "The first weight refers to the training set, and the second to the test set.\n",
        "They both need to be values between 0 and 1, and sum up to 1.\n",
        "\"\"\"\n",
        "\n",
        "missing_label_substitution = None\n",
        "\"\"\"string: how to fill the missing values in target label\n",
        "\n",
        "Setting the variable to \"zero\" means substitute empty target cell with 0; a\n",
        "value of None means removing the entire row where datum is missing.\n",
        "\"\"\"\n",
        "\n",
        "nan_substitution_strategy = 'mean'\n",
        "\"\"\"string: The strategy to use to remove NaN.\n",
        "\n",
        "The string can assume the following values:\n",
        "    - \"mean\"\n",
        "    - \"median\"\n",
        "\"\"\"\n",
        "\n",
        "pca_new_space_dimension = 10\n",
        "\"\"\"int: Number of features to preserve with PCA \"\"\"\n",
        "\n",
        "cross_validation_folds = 5\n",
        "cross_validation_internal_split_seed = 1234\n",
        "cross_validation_alphas = [*np.logspace(-8, 0, 7), *np.logspace(0.1, 3, 8)]\n",
        "\"\"\"Number of folds and list of alphas to use in cross validation.\n",
        "\n",
        "Alphas must be a list of values, each of which will be used as ridge regression\n",
        "hyperparameter.\n",
        "\n",
        "The seed is used for internal splitting of the folds.\n",
        "\"\"\"\n",
        "\n",
        "caching = True\n",
        "\"\"\"bool: Decide to cache or not some RDDs. \"\"\"\n",
        "\n",
        "outlier_strategy = None\n",
        "\"\"\"string: The strategy used to trim outliers\n",
        "\n",
        "The admitted string are:\n",
        "    - \"iqr\" = use the interquartile range.\n",
        "\n",
        "If set to None, no trim is performed.\n",
        "\"\"\"\n",
        "\n",
        "number_of_partitions = sc.defaultParallelism * 2\n",
        "\"\"\"int: Define the level of parallelelism to use\n",
        "\n",
        "By default, `defaultParallelism` corresponds to the number of cores of the machine.\n",
        "As a rule of thumbs, a good choice for the number of partitions is two/three times\n",
        "the amount of cores.\n",
        "\"\"\"\n",
        "\n",
        "verbose = False\n",
        "\"\"\"bool: Print information about some expensive operation.\n",
        "\n",
        "This flag is used as a guard for some cpu expensive operation: set it to False\n",
        "to increase the execution time.\n",
        "\"\"\"\n",
        "\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvkHfQ1-R6xl"
      },
      "source": [
        "# Load data into DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24YoM2BpR6xl"
      },
      "source": [
        "# base_path = \"/home/stefano/codes/progetto_amd_prova/\"\n",
        "base_path = '/content/data/'\n",
        "df = spark.read.csv(base_path + dataset_to_load,\n",
        "                    inferSchema=True,\n",
        "                    header=True) \\\n",
        "               .sample(withReplacement=False,\n",
        "                       fraction=dataset_sample_fraction,\n",
        "                       seed=dataset_sample_seed)\n",
        "\n",
        "if verbose:\n",
        "    print('The loaded dataframe has the following shape: {} rows - {} columns'\n",
        "          .format(df.count(), len(df.columns)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqtbzINsR6xp"
      },
      "source": [
        "if verbose: df.describe(target_label).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOvfidifR6xs"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "## remove unused columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP7PhmxpR6xt"
      },
      "source": [
        "# remove banned columns\n",
        "print('The following columns cannot be used in prediction: {}'.format(forbidden_columns))\n",
        "df = df.drop(*forbidden_columns)\n",
        "print('The new total number of column is {}'.format(len(df.columns)))\n",
        "\n",
        "# remove columns of string type\n",
        "print('Remove columns which have string type')\n",
        "str_type_cols = [t[0] for t in df.dtypes if t[1] == 'string']\n",
        "print('The following columns are going to be removed: {}'.format(str_type_cols))\n",
        "df = df.drop(*str_type_cols)\n",
        "print('The new total number of column is {}'.format(len(df.columns)))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYAdKIQyR6xw"
      },
      "source": [
        "import pyspark.sql.functions as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eau0m5ypR6xz"
      },
      "source": [
        "# cast all values to float\n",
        "print('Casting all values to float\\n')\n",
        "df = df.select(*(F.col(c).cast(\"float\").alias(c) for c in df.columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldhiNewsR6x2"
      },
      "source": [
        "## manage missing values in target label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pJqUXkCR6x3"
      },
      "source": [
        "if missing_label_substitution == \"zero\":\n",
        "    df = df.fillna({target_label: 0})\n",
        "else:\n",
        "    df = df.filter(df[target_label].isNotNull())\n",
        "if verbose: \n",
        "    print(\"The new total number of rows is {}\".format(df.count()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztJ7sjy_cLQP"
      },
      "source": [
        "# Split dataframe into training and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6at2i3scQBV"
      },
      "source": [
        "df_train, df_test = df.randomSplit(split_weights,\n",
        "                                   seed=split_seed)\n",
        "\n",
        "print(\"Dimensions: training set = {}, test set = {}\"\n",
        "      .format(df_train.count(),\n",
        "              df_test.count()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMUeJHb5XT4v"
      },
      "source": [
        "## find out and trim the outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ1SDC04XT4v"
      },
      "source": [
        "def compute_quantiles(dataframe, percentiles, approx):\n",
        "    return dataframe.approxQuantile(target_label,\n",
        "                                    percentiles,\n",
        "                                    approx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPfdLxnYXT4v"
      },
      "source": [
        "if outlier_strategy == \"iqr\":\n",
        "    approx = 0.01\n",
        "\n",
        "    percentiles = [.25, .75]\n",
        "\n",
        "    quantiles = compute_quantiles(df_train,\n",
        "                                  percentiles,\n",
        "                                  approx)\n",
        "\n",
        "    min_label = df_train.select(F.min(target_label).alias(\"MIN\")) \\\n",
        "                        .limit(1) \\\n",
        "                        .collect()[0].MIN\n",
        "    max_label = df_train.select(F.max(target_label).alias(\"MAX\")) \\\n",
        "                        .limit(1) \\\n",
        "                        .collect()[0].MAX\n",
        "\n",
        "    iqr = quantiles[1] - quantiles[0]\n",
        "    lower_whisker = quantiles[0] - iqr * 1.5\n",
        "    upper_whisker = quantiles[1] + iqr * 1.5\n",
        "\n",
        "    lower_whisker = lower_whisker if lower_whisker > min_label else min_label\n",
        "    upper_whisker = upper_whisker if upper_whisker < max_label else max_label\n",
        "\n",
        "\n",
        "    print(\"quantiles: {}\\n whiskers: {},{}\"\n",
        "          .format(quantiles,\n",
        "                  lower_whisker,\n",
        "                  upper_whisker))\n",
        "\n",
        "    df_train = df_train.filter(df_train[target_label] > lower_whisker) \\\n",
        "                       .filter(df_train[target_label] < upper_whisker)\n",
        "\n",
        "    if verbose:\n",
        "        df_train.describe(target_label).show()\n",
        "\n",
        "else:\n",
        "    print(\"no outlier trimming performed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imaaiHnAR6x6"
      },
      "source": [
        "## substitute NaN values with the mean of corresponding column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caG0FEbcR6x7"
      },
      "source": [
        "from pyspark.ml.feature import Imputer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmMzwRbiR6yA"
      },
      "source": [
        "%%time\n",
        "# create the imputer\n",
        "imputer = Imputer(strategy=nan_substitution_strategy,\n",
        "                  inputCols=df.columns,\n",
        "                  outputCols=df.columns)\n",
        "\n",
        "# transoform the entire dataframe according to the model\n",
        "imputer_model = imputer.fit(df_train)\n",
        "df_train = imputer_model.transform(df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOa8F2CHR6yD"
      },
      "source": [
        "# Tranform dataset\n",
        "\n",
        "First create a new column which is the union of all other columns except the target label.\n",
        "Then scale all features, removing mean and stddev.\n",
        "Finally, performe a PCA reduction over the scaled features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrfhzKhyR6yE"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X3flQxkR6yG"
      },
      "source": [
        "%%time\n",
        "\n",
        "fet_vec_output_col = \"features\"\n",
        "std_scaler_output_col = \"features_scaled\"\n",
        "final_output_col = \"pca_features\"\n",
        "\n",
        "features = df_train.columns\n",
        "features.remove(target_label)\n",
        "\n",
        "features_vec = VectorAssembler(inputCols=features,\n",
        "                               outputCol=fet_vec_output_col)\n",
        "\n",
        "df_train = features_vec.transform(df_train)\n",
        "\n",
        "standard_scaler = StandardScaler(inputCol=fet_vec_output_col,\n",
        "                                     outputCol=std_scaler_output_col)\n",
        "\n",
        "std_scaler_model = standard_scaler.fit(df_train)\n",
        "df_train = std_scaler_model.transform(df_train)\n",
        "\n",
        "\n",
        "pca = PCA(k=pca_new_space_dimension,\n",
        "          inputCol=std_scaler_output_col,\n",
        "          outputCol=final_output_col)\n",
        "\n",
        "pca_model = pca.fit(df_train)\n",
        "df_train = pca_model.transform(df_train)\n",
        "\n",
        "\n",
        "df_train = df_train.select([target_label, final_output_col])\n",
        "\n",
        "if verbose:\n",
        "    print(\"Dataframe of target/features values has the following shape: {} rows, {} cols\"\n",
        "         .format(df_train.count(), len(df_train.columns)))\n",
        "\n",
        "df_train.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ8tO3l7XT4z"
      },
      "source": [
        "print(\"PCA retained variance: {}\"\n",
        "     .format(sum(pca_model.explainedVariance)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-gWFmZGdXDy"
      },
      "source": [
        "# Apply changes to test dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9581sM_dh36"
      },
      "source": [
        "df_test = imputer_model.transform(df_test)\n",
        "\n",
        "df_test = features_vec.transform(df_test)\n",
        "df_test = std_scaler_model.transform(df_test)\n",
        "\n",
        "df_test = pca_model.transform(df_test)\n",
        "\n",
        "df_test = df_test.select([target_label, final_output_col])\n",
        "\n",
        "df_test.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UYPRW26R6yJ"
      },
      "source": [
        "# Create RDD from DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWOF4CaeR6yK"
      },
      "source": [
        "training_set = df_train.rdd\n",
        "test_set = df_test.rdd\n",
        "\n",
        "print(training_set.take(3))\n",
        "print(test_set.take(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbqVyHnHR6yM"
      },
      "source": [
        "from pyspark.mllib.feature import LabeledPoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP_sKcBYR6yP"
      },
      "source": [
        "def to_labeledPoint(row, label_col, features_col):\n",
        "    label = row[label_col]\n",
        "    # DenseVector can't be used to create a LabeledPoint\n",
        "    features = row[features_col].values\n",
        "    return LabeledPoint(label, features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUWB-0yzR6yS"
      },
      "source": [
        "training_set = training_set.map(lambda row: to_labeledPoint(row,\n",
        "                                                            target_label,\n",
        "                                                            final_output_col))\n",
        "\n",
        "test_set = test_set.map(lambda row: to_labeledPoint(row,\n",
        "                                                    target_label,\n",
        "                                                    final_output_col))\n",
        "if caching: test_set.cache()\n",
        "\n",
        "training_set = training_set.repartition(12)\n",
        "test_set = test_set.repartition(12)\n",
        "\n",
        "print(training_set.take(3))\n",
        "print(test_set.take(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSrGRDK4R6yX"
      },
      "source": [
        "# Function estimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNkMPN9NR6yY"
      },
      "source": [
        "def squared_err(expected, prediction):\n",
        "    '''Compute the squared error, i.e. the squared difference between the inputs'''    \n",
        "    return (expected - prediction) ** 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XCMAWxdR6ya"
      },
      "source": [
        "def mean_squared_error(rdd):\n",
        "    '''Compute the mean squared error of the given RDD.\n",
        "    \n",
        "    The RDD must be of the form RDD[(float, float)]\n",
        "    '''\n",
        "    return rdd.map(lambda p: squared_err(*p)).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_DvF-TYR6yd"
      },
      "source": [
        "def root_mean_squared_error(rdd):\n",
        "    '''Computes the root mean squared error of the given RDD.\n",
        "    \n",
        "    The RDD in input must be of the form RDD[(float, float)]'''\n",
        "    return np.sqrt(mean_squared_error(rdd))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD6BGLFtR6yf"
      },
      "source": [
        "def r2(rdd):\n",
        "    '''Computes the R2 coefficient of determination\n",
        "    \n",
        "    The RDD in input must be of the form RDD[(float, float)], where the first float\n",
        "    is the actual value, and the second float is the predicted value.\n",
        "    '''\n",
        "    mean_ = rdd.map(lambda t: t[0]).mean()\n",
        "    sum_squares = rdd.map(lambda t: (t[0] - mean_)**2).sum()\n",
        "    residual_sum_squares = rdd.map(lambda t: squared_err(*t)).sum()\n",
        "    return 1 - (residual_sum_squares / sum_squares)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP2Cn2juKZ4p"
      },
      "source": [
        "# Baseline algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcxqxm0pKfh3"
      },
      "source": [
        "# get label main\n",
        "label_mean = training_set.map(lambda lp: lp.label).mean()\n",
        "\n",
        "# create and RDD of (label, prediction)\n",
        "baseline_train_prediction_rdd = training_set.map(lambda lp: (lp.label, label_mean))\n",
        "\n",
        "# predict test set\n",
        "baseline_test_prediction_rdd = test_set.map(lambda lp: (lp.label, label_mean))\n",
        "\n",
        "## compute stats\n",
        "# mse\n",
        "baseline_train_mse = mean_squared_error(baseline_train_prediction_rdd)\n",
        "baseline_test_mse = mean_squared_error(baseline_test_prediction_rdd)\n",
        "\n",
        "# rmse\n",
        "baseline_train_rmse = root_mean_squared_error(baseline_train_prediction_rdd)\n",
        "baseline_test_rmse = root_mean_squared_error(baseline_test_prediction_rdd)\n",
        "\n",
        "print('BASELINE VALUES\\n'\n",
        "      '\\tMSE:\\n\\ttrain: {:.5e}\\n\\ttest: {:.5e}'.format(baseline_train_mse, baseline_test_mse))\n",
        "print('\\tRMSE:\\n\\ttrain: {:.5e}\\n\\ttest: {:.5e}'.format(baseline_train_rmse, baseline_test_rmse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vANtFzZbR6yi"
      },
      "source": [
        "# Ridge regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OXUBCT1R6yk"
      },
      "source": [
        "def outer_product(lp):\n",
        "    '''Computes the outer product of the features of the LabeledPoint with themselves.\n",
        "    \n",
        "    This method is meant to be used inside a map function of an RDD[LabeledPoint]\n",
        "    \n",
        "    The outer product of the input is defined as:\n",
        "        lp.features x lp.features'\n",
        "    returning an m x m matrix, with m dimension of the vector lp.features\n",
        "    '''\n",
        "    return np.array([np.dot(i,lp.features) for i in lp.features])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGGLPX5aR6yn"
      },
      "source": [
        "def outer_sum(m1, m2):\n",
        "    '''Sums two matrixes'''\n",
        "    return m1+m2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t88TiV0nR6yq"
      },
      "source": [
        "def stripes_product(rdd):\n",
        "    '''Multiplies the design matrix with the feature vector of corresponding label\n",
        "    \n",
        "    The input must be and RDD[LabeledPoint]\n",
        "    \n",
        "    Computes a matrix-vector product, multipling each feature vector with the corresponding scalar label.\n",
        "    It is not necessary to transponse the matrix.\n",
        "    '''\n",
        "    return rdd.map(lambda lp: lp.label*lp.features).reduce(lambda v1, v2: v1+v2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbwr0G5LR6yt"
      },
      "source": [
        "def ridge_regression(rdd, alpha=1):\n",
        "    '''Creates the ERM weights according to Ridge Regression model\n",
        "    \n",
        "    The RDD in input must be of the form RDD[LabeledPoint]\n",
        "    '''\n",
        "    A = rdd.map(outer_product).reduce(outer_sum)\n",
        "    np.fill_diagonal(A, A.diagonal()+alpha)\n",
        "    V = stripes_product(rdd)\n",
        "    return np.linalg.inv(A).dot(V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTdruNw1R6yv"
      },
      "source": [
        "def predict(weights, features):\n",
        "    '''Predicts the output according to the given weights and features'''\n",
        "    return np.dot(weights, features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eudJRmxsZhW9"
      },
      "source": [
        "def cross_validation(rdd, k, alpha_set, m=None, seed=None, verbose=False):\n",
        "    '''Computes the cross validation over the given RDD\n",
        "    \n",
        "    The RDD in input must be of the form RDD[LabeledPoint]\n",
        "    \n",
        "    The returned value is a dictonary, where the keys are the given alpha inside\n",
        "    alpha_set, and the values are a list of tuple, where each tuple is of the form:\n",
        "            (fold error, fold r2 score)\n",
        "    '''\n",
        "    # split the rdd into k folds\n",
        "    split_weights = np.repeat(1/k, k)\n",
        "    folds = rdd.randomSplit(split_weights, seed=seed)\n",
        "    \n",
        "    if m is None:\n",
        "        m = rdd.count()\n",
        "        if verbose: print(\"count computed\")\n",
        "\n",
        "    scaling_factor = k / m\n",
        "    risks = {a: [] \n",
        "             for a \n",
        "             in alpha_set}\n",
        "    \n",
        "    for i in range(k):\n",
        "        Di = folds[i] # validation fold\n",
        "        if caching: Di.cache()\n",
        "        Si = folds[:i] + folds[i+1:] # all folds except Di\n",
        "        training_part = sc.union(Si) # creates testing RDD\n",
        "        training_part = training_part.repartition(12)\n",
        "        \n",
        "        for alpha in alpha_set:\n",
        "            _w = ridge_regression(training_part, alpha)\n",
        "            predicted_rdd_ = Di.map(lambda lp: (lp.label, predict(_w, lp.features)))\n",
        "            fold_r2 = r2(predicted_rdd_)\n",
        "            fold_error = scaling_factor * predicted_rdd_.map(lambda t: squared_err(*t)) \\\n",
        "                                                        .sum()\n",
        "            risks[alpha].append((fold_error, fold_r2))\n",
        "            if verbose: print('.', end='')\n",
        "        if caching: Di.unpersist()\n",
        "        print(\" fold {} computed\".format(i+1))\n",
        "\n",
        "    return risks\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXyOr4Zxdu8q"
      },
      "source": [
        "%%time\n",
        "\n",
        "# call once for all CV\n",
        "m = training_set.count()\n",
        "\n",
        "risks = cross_validation(training_set,\n",
        "                         cross_validation_folds, \n",
        "                         cross_validation_alphas, \n",
        "                         m=m, \n",
        "                         seed=cross_validation_internal_split_seed, \n",
        "                         verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeEAoqwyZEHh"
      },
      "source": [
        "cv_risks = {\n",
        "    alpha: [t[0] for t in risk]\n",
        "    for alpha, risk \n",
        "    in risks.items()\n",
        "}\n",
        "\n",
        "scores = {\n",
        "    alpha: [t[1] for t in risk]\n",
        "    for alpha, risk \n",
        "    in risks.items()\n",
        "}\n",
        "\n",
        "# computes CV risks\n",
        "for k, v in cv_risks.items():\n",
        "    cv_risks[k] = sum(v) / len(v)\n",
        "\n",
        "min_alpha, min_risk = min(cv_risks.items(), key=lambda t: t[1])\n",
        "\n",
        "print(\"The minimum risk is {:.5E}, given by alpha={:.6}\"\n",
        "      .format(min_risk, min_alpha))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ytsCb6XjDlq"
      },
      "source": [
        "import seaborn as sbn\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSXi4y_keqkg"
      },
      "source": [
        "Plot the expected risk against alpha value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPKinGPRjGv-"
      },
      "source": [
        "alpha_values = list(cv_risks.keys())\n",
        "expected_risks = list(cv_risks.values())\n",
        "\n",
        "_ = sbn.lineplot(x=alpha_values,\n",
        "                 y=expected_risks)\n",
        "ax = sbn.scatterplot(x=alpha_values,\n",
        "                     y=expected_risks)\n",
        "ax.set_xlabel(\"alpha\")\n",
        "_ = ax.set_ylabel(\"risk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kATGS1xde6V5"
      },
      "source": [
        "Plot the R2 score against the alpha value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoEqeGj9ZEHt"
      },
      "source": [
        "mean_scores = [np.mean(i)\n",
        "               for i\n",
        "               in scores.values()]\n",
        "\n",
        "_ = sbn.lineplot(x=list(scores.keys()),\n",
        "             y=mean_scores)\n",
        "ax = sbn.scatterplot(x=list(scores.keys()),\n",
        "             y=mean_scores)\n",
        "ax.set_xlabel(\"alpha\")\n",
        "_ = ax.set_ylabel(\"R2 score\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VI8YQ5AZEHz"
      },
      "source": [
        "weights = ridge_regression(training_set, alpha=min_alpha)\n",
        "#print(weights)\n",
        "#print(\"weights computed\")\n",
        "\n",
        "# prediction the same training set\n",
        "ridge_train_prediction_rdd = training_set.map(lambda lp:\n",
        "                                              (lp.label, predict(weights,\n",
        "                                                                 lp.features)))\n",
        "\n",
        "# prediction onto test set\n",
        "ridge_test_prediction_rdd = test_set.map(lambda lp:\n",
        "                                         (lp.label, predict(weights,\n",
        "                                                            lp.features)))\n",
        "\n",
        "# compute stats\n",
        "# mse\n",
        "ridge_train_mse = mean_squared_error(ridge_train_prediction_rdd)\n",
        "ridge_test_mse = mean_squared_error(ridge_test_prediction_rdd)\n",
        "\n",
        "# rmse\n",
        "ridge_train_rmse = root_mean_squared_error(ridge_train_prediction_rdd)\n",
        "ridge_test_rmse = root_mean_squared_error(ridge_test_prediction_rdd)\n",
        "\n",
        "# r2\n",
        "ridge_train_r2 = r2(ridge_train_prediction_rdd)\n",
        "ridge_test_r2 = r2(ridge_test_prediction_rdd)\n",
        "\n",
        "print('RIDGE REGRESSION\\n'\n",
        "      'Train\\n'\n",
        "      '\\tMSE: {:.5e}\\n'\n",
        "      '\\tRMSE: {:.5e}\\n'\n",
        "      '\\tR2: {:.3}'\n",
        "      .format(ridge_train_mse,\n",
        "              ridge_train_rmse,\n",
        "              ridge_train_r2))\n",
        "\n",
        "print('Test\\n'\n",
        "      '\\tMSE: {:.5e}\\n'\n",
        "      '\\tRMSE: {:.5e}\\n'\n",
        "      '\\tR2: {:.3}'\n",
        "      .format(ridge_test_mse,\n",
        "              ridge_test_rmse,\n",
        "              ridge_test_r2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkVoPwfNe-vq"
      },
      "source": [
        "Plot the prediction of ridge regression model against the true value of the label.\n",
        "\n",
        "The smaller and the lighter the point, the better the prediction is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcIyEHY4ZEH3"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "predictions = np.asarray(\n",
        "    ridge_test_prediction_rdd \\\n",
        "        .map(lambda t: t[1]) \\\n",
        "        .collect())\n",
        "\n",
        "actual = np.asarray(\n",
        "    ridge_test_prediction_rdd \\\n",
        "        .map(lambda t: t[0]) \\\n",
        "        .collect())\n",
        "\n",
        "error = np.asarray(\n",
        "    ridge_test_prediction_rdd \\\n",
        "        .map(lambda t: squared_err(*t)) \\\n",
        "        .collect())\n",
        "\n",
        "    \n",
        "ax = sbn.scatterplot(x=predictions,\n",
        "                     y=actual,\n",
        "                     hue=error,\n",
        "                     size=error,\n",
        "                     sizes=(10, 250))\n",
        "\n",
        "ax.set_ylabel(\"true values\")\n",
        "_ = ax.set_xlabel(\"predictions\")\n",
        "ax.legend().remove()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dixcLZY3XT49"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}